{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9897426,"sourceType":"datasetVersion","datasetId":6079480},{"sourceId":165737,"sourceType":"modelInstanceVersion","modelInstanceId":141023,"modelId":163629}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Joint Intent Classification and Slot Filliing with Transformers\n\n**Goal**\n* Fine-tune a pretrained transformer-based neural network model to convert a user qeury expressed in English into a representation that is structured enough to be processed by an automated service.\n\nHere is an example of interpretation computed by such a Natural Language Understanding system:\n    \n    >>> nlu(''क्या आज मनाली में बर्फबारी होगी?'\",\n            tokenizer, joint_model, intent_names, slot_names)\n    {\n        {'intent': 'GetWeather',\r        \n 'slots': {'condition_description': 'snowfalls',        \r\n  'city':Manalili'        ,\r\n  'timeRange': 'today?'}}\n  nge\n     \nWe will shw hhow to train a such \"sequence classification\" and \"token classification\" joint model on a voice command dataset published by snips.ai. This notebook is a partial reproduction of some of the results presented in this paper: BERT for Joint Intent Classification and Shot Filling, Qian Chen, Zhu Zhuo, Wen Wang [link](https://arxiv.org/abs/1902.10909).","metadata":{}},{"cell_type":"code","source":"!pip install transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T14:10:02.655190Z","iopub.status.idle":"2024-11-17T14:10:02.655531Z","shell.execute_reply.started":"2024-11-17T14:10:02.655363Z","shell.execute_reply":"2024-11-17T14:10:02.655380Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load packages\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom pathlib import Path\nfrom transformers import BertTokenizer, TFBertModel\nfrom urllib.request import urlretrieve\n\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.metrics import SparseCategoricalAccuracy\nfrom tensorflow.keras.optimizers import Adam","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T14:10:03.058620Z","iopub.execute_input":"2024-11-17T14:10:03.059045Z","iopub.status.idle":"2024-11-17T14:10:21.350557Z","shell.execute_reply.started":"2024-11-17T14:10:03.059007Z","shell.execute_reply":"2024-11-17T14:10:21.349559Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## The data\n\nWe will use a speed command dataset collected, annotated and published by French startup snips.ai (bought in 2019 by Audio device manufacturer Sonos). The original dataset comes in [YAML format with inline markdown annotations](https://snips-nlu.readthedocs.io/en/latest/dataset.html). Instead, we will use a preprocessed variant with token level B-I-O annotations closer to the representation our model will predict. This variant of the snips dataset was prepared by [Su Zhu](https://github.com/sz128).","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false,"jupyter":{"outputs_hidden":true}}},{"cell_type":"markdown","source":"Let's have a look at the first lines from the training set.","metadata":{}},{"cell_type":"code","source":"lines_train = Path('/kaggle/input/softcomp/train.txt').read_text('utf-8').strip().splitlines()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T14:10:21.376402Z","iopub.execute_input":"2024-11-17T14:10:21.376719Z","iopub.status.idle":"2024-11-17T14:10:21.420947Z","shell.execute_reply.started":"2024-11-17T14:10:21.376688Z","shell.execute_reply":"2024-11-17T14:10:21.420223Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def parse_line(line):\n    utterance_data, intent_label = line.split(\" <=> \")\n    items = utterance_data.split()\n    words = [item.rsplit(':', 1)[0] for item in items]\n    word_labels = [item.rsplit(':', 1)[1] for item in items]\n    return {\n        'intent_label': intent_label,\n        'words': \" \".join(words),\n        'words_label': \" \".join(word_labels),\n        'length': len(words)\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T14:10:21.421984Z","iopub.execute_input":"2024-11-17T14:10:21.422322Z","iopub.status.idle":"2024-11-17T14:10:21.428110Z","shell.execute_reply.started":"2024-11-17T14:10:21.422289Z","shell.execute_reply":"2024-11-17T14:10:21.427196Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"parse_line(lines_train[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T14:10:21.429311Z","iopub.execute_input":"2024-11-17T14:10:21.429662Z","iopub.status.idle":"2024-11-17T14:10:21.439697Z","shell.execute_reply.started":"2024-11-17T14:10:21.429629Z","shell.execute_reply":"2024-11-17T14:10:21.438874Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'intent_label': 'AddToPlaylist',\n 'words': 'Add Don and Sherri to my Meditate to Sounds of Nature playlist',\n 'words_label': 'O B-entity_name I-entity_name I-entity_name O B-playlist_owner B-playlist I-playlist I-playlist I-playlist I-playlist O',\n 'length': 12}"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"This utterance is a voice command of type \"AddToPlayist\" with annotations:\n* an entity-name: \"Don and Sherri\",\n* a playlist: \"Medidate to Sounds of Nature\".\n\nThe goal of this project is to build a baseline Natural Understanding model to analyse such voice commands and predict:\n* the intent of the speaker: the sentence level class label (\"AddToPlaylist\");\n* extract the interesting slots (typed named entities) from the sentence by performing word level classification using the B-I-O tags as target classes. This second task is often referred to as \"NER\" (Named Entity Recognition) in the NLP litterature. Alternatively, this is also known as \"slot filling\" when we expect a fixed set of named entity per sentence of a given class.\n\nThe list of possible classes for the sentence level and the word level classification problems are given as:","metadata":{}},{"cell_type":"markdown","source":"\"POI\" stands for \"Point of Interest\". Let's parse all the lines and store the results in a Pandas dataframes.","metadata":{}},{"cell_type":"code","source":"parsed = [parse_line(line) for line in lines_train]\ndf_train = pd.DataFrame([p for p in parsed if p is not None])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T14:10:21.440691Z","iopub.execute_input":"2024-11-17T14:10:21.440966Z","iopub.status.idle":"2024-11-17T14:10:21.584860Z","shell.execute_reply.started":"2024-11-17T14:10:21.440937Z","shell.execute_reply":"2024-11-17T14:10:21.584086Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Print some lines of the training set\ndf_train.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T14:10:21.585998Z","iopub.execute_input":"2024-11-17T14:10:21.586684Z","iopub.status.idle":"2024-11-17T14:10:21.603073Z","shell.execute_reply.started":"2024-11-17T14:10:21.586640Z","shell.execute_reply":"2024-11-17T14:10:21.602226Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"    intent_label                                              words  \\\n0  AddToPlaylist  Add Don and Sherri to my Meditate to Sounds of...   \n1  AddToPlaylist  put United Abominations onto my rare groove pl...   \n2  AddToPlaylist  add the tune by misato watanabe to the Trapeo ...   \n3  AddToPlaylist  add this artist to my this is miguel bosé play...   \n4  AddToPlaylist  add heresy and the hotel choir to the evening ...   \n\n                                         words_label  length  \n0  O B-entity_name I-entity_name I-entity_name O ...      12  \n1  O B-entity_name I-entity_name O B-playlist_own...       8  \n2  O O B-music_item O B-artist I-artist O O B-pla...      10  \n3  O O B-music_item O B-playlist_owner B-playlist...      10  \n4  O B-entity_name I-entity_name I-entity_name I-...      11  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>intent_label</th>\n      <th>words</th>\n      <th>words_label</th>\n      <th>length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AddToPlaylist</td>\n      <td>Add Don and Sherri to my Meditate to Sounds of...</td>\n      <td>O B-entity_name I-entity_name I-entity_name O ...</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>AddToPlaylist</td>\n      <td>put United Abominations onto my rare groove pl...</td>\n      <td>O B-entity_name I-entity_name O B-playlist_own...</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>AddToPlaylist</td>\n      <td>add the tune by misato watanabe to the Trapeo ...</td>\n      <td>O O B-music_item O B-artist I-artist O O B-pla...</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AddToPlaylist</td>\n      <td>add this artist to my this is miguel bosé play...</td>\n      <td>O O B-music_item O B-playlist_owner B-playlist...</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>AddToPlaylist</td>\n      <td>add heresy and the hotel choir to the evening ...</td>\n      <td>O B-entity_name I-entity_name I-entity_name I-...</td>\n      <td>11</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# Count the number of lines by intent label\ndf_train.intent_label.value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T14:10:21.604144Z","iopub.execute_input":"2024-11-17T14:10:21.604467Z","iopub.status.idle":"2024-11-17T14:10:21.620917Z","shell.execute_reply.started":"2024-11-17T14:10:21.604436Z","shell.execute_reply":"2024-11-17T14:10:21.619701Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"intent_label\nGetWeather              1900\nPlayMusic               1900\nBookRestaurant          1873\nSearchScreeningEvent    1859\nRateBook                1856\nSearchCreativeWork      1854\nAddToPlaylist           1842\nName: count, dtype: int64"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Histogram of sentence lengths\ndf_train.hist('length', bins=30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T14:10:21.624585Z","iopub.execute_input":"2024-11-17T14:10:21.624844Z","iopub.status.idle":"2024-11-17T14:10:21.962318Z","shell.execute_reply.started":"2024-11-17T14:10:21.624815Z","shell.execute_reply":"2024-11-17T14:10:21.961382Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"array([[<Axes: title={'center': 'length'}>]], dtype=object)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# Get validation and test set\nlines_validation = Path('/kaggle/input/softcomp/valid.txt').read_text('utf-8').strip().splitlines()\nlines_test = Path('/kaggle/input/softcomp/test.txt').read_text('utf-8').strip().splitlines()\nlines_train=Path('/kaggle/input/softcomp/train.txt').read_text('utf-8').strip().splitlines()\ndf_validation = pd.DataFrame([parse_line(line) for line in lines_validation])\ndf_test = pd.DataFrame([parse_line(line) for line in lines_test])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T14:10:21.963611Z","iopub.execute_input":"2024-11-17T14:10:21.963922Z","iopub.status.idle":"2024-11-17T14:10:22.021491Z","shell.execute_reply.started":"2024-11-17T14:10:21.963889Z","shell.execute_reply":"2024-11-17T14:10:22.020752Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## Intent classification (sentence level)\n\nLet's ignore the slot filling task for now and let's try to build a sentence level classifier by fine-tuning a pre-trained Transformer-based model using the `huggingface/transformers` package that provides both Tensorflow/Keras and Pytorch APIs.\n\n### The BERT tokenizer\n\nFirst let's load a pre-trained tokenizer and test it on a test sentence from the training set.","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer, TFBertModel\n\n# Specify the path to your local model directory\nmodel_path = \"/kaggle/input/bert/tensorflow2/default/1/bert\"\n\n# Load the tokenizer and model from the local directory\ntokenizer = BertTokenizer.from_pretrained(model_path)\nmodel = TFBertModel.from_pretrained(model_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T14:10:22.022612Z","iopub.execute_input":"2024-11-17T14:10:22.022973Z","iopub.status.idle":"2024-11-17T14:10:25.943420Z","shell.execute_reply.started":"2024-11-17T14:10:22.022931Z","shell.execute_reply":"2024-11-17T14:10:25.942477Z"}},"outputs":[{"name":"stderr","text":"Some layers from the model checkpoint at /kaggle/input/bert/tensorflow2/default/1/bert were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at /kaggle/input/bert/tensorflow2/default/1/bert.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"first_sentence = df_train.iloc[0]['words']\nprint(first_sentence)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T14:10:25.944889Z","iopub.execute_input":"2024-11-17T14:10:25.945225Z","iopub.status.idle":"2024-11-17T14:10:25.950083Z","shell.execute_reply.started":"2024-11-17T14:10:25.945182Z","shell.execute_reply":"2024-11-17T14:10:25.949203Z"}},"outputs":[{"name":"stdout","text":"Add Don and Sherri to my Meditate to Sounds of Nature playlist\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"tokenizer.tokenize(first_sentence)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T14:10:25.951248Z","iopub.execute_input":"2024-11-17T14:10:25.951538Z","iopub.status.idle":"2024-11-17T14:10:25.963985Z","shell.execute_reply.started":"2024-11-17T14:10:25.951504Z","shell.execute_reply":"2024-11-17T14:10:25.962699Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"['Ad',\n '##d',\n 'Don',\n 'and',\n 'She',\n '##rri',\n 'to',\n 'my',\n 'Me',\n '##dit',\n '##ate',\n 'to',\n 'Sounds',\n 'of',\n 'Nature',\n 'play',\n '##list']"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"Notive that BERT uses subword tokens. So the length of the tokenized sentence is likely to be larger than the number of words in the sentence. It is particularly interesting to use subword tokenization sentence for general purpose language models such as BERT because it should be possible to generalize the model and then to fine-tuned it to be a specialized one.\n\nEach token string is mapped to a unique integer id that makes it fast to lookup the right column in the input layer token embedding.","metadata":{}},{"cell_type":"markdown","source":"To perform transfer learning, we will need to work with padded sequences. So, they all have the same sizes. The above histograms, shows that after tokenization, $43$ tokens are enough to represent all the voice commands in the training set.\n\nThe mapping can be introspected in the `tokenizer.vocab` attribute.","metadata":{}},{"cell_type":"code","source":"print(f'Vocabulary size: {tokenizer.vocab_size} words.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T14:10:25.965577Z","iopub.execute_input":"2024-11-17T14:10:25.965878Z","iopub.status.idle":"2024-11-17T14:10:25.974986Z","shell.execute_reply.started":"2024-11-17T14:10:25.965832Z","shell.execute_reply":"2024-11-17T14:10:25.974116Z"}},"outputs":[{"name":"stdout","text":"Vocabulary size: 28996 words.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"def encode_dataset(tokenizer, text_sequences, max_length):\n    token_ids = np.zeros(shape=(len(text_sequences), max_length),\n                         dtype=np.int32)\n    for i, text_sequence in enumerate(text_sequences):\n        encoded = tokenizer.encode(text_sequence)\n        token_ids[i, 0:len(encoded)] = encoded\n    attention_masks = (token_ids != 0).astype(np.int32)\n    \n    return {'input_ids': token_ids, 'attention_mask': attention_masks}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T14:10:25.976065Z","iopub.execute_input":"2024-11-17T14:10:25.976456Z","iopub.status.idle":"2024-11-17T14:10:25.984867Z","shell.execute_reply.started":"2024-11-17T14:10:25.976404Z","shell.execute_reply":"2024-11-17T14:10:25.983905Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"encoded_train = encode_dataset(tokenizer, df_train['words'], 45)\nencoded_validation = encode_dataset(tokenizer, df_validation['words'], 45)\nencoded_test = encode_dataset(tokenizer, df_test['words'], 45)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T14:10:25.986026Z","iopub.execute_input":"2024-11-17T14:10:25.986370Z","iopub.status.idle":"2024-11-17T14:10:30.771731Z","shell.execute_reply.started":"2024-11-17T14:10:25.986337Z","shell.execute_reply":"2024-11-17T14:10:30.770915Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"encoded_train['input_ids']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T14:10:30.772994Z","iopub.execute_input":"2024-11-17T14:10:30.773697Z","iopub.status.idle":"2024-11-17T14:10:30.780514Z","shell.execute_reply.started":"2024-11-17T14:10:30.773650Z","shell.execute_reply":"2024-11-17T14:10:30.779608Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"array([[  101, 24930,  1181, ...,     0,     0,     0],\n       [  101,  1508,  1244, ...,     0,     0,     0],\n       [  101,  5194,  1103, ...,     0,     0,     0],\n       ...,\n       [  101, 27640,  1116, ...,     0,     0,     0],\n       [  101,  5979,  6608, ...,     0,     0,     0],\n       [  101,  1327,  2523, ...,     0,     0,     0]], dtype=int32)"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"encoded_train['attention_mask']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T14:10:30.781701Z","iopub.execute_input":"2024-11-17T14:10:30.782050Z","iopub.status.idle":"2024-11-17T14:10:30.799079Z","shell.execute_reply.started":"2024-11-17T14:10:30.782008Z","shell.execute_reply":"2024-11-17T14:10:30.798291Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"array([[1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       ...,\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)"},"metadata":{}}],"execution_count":20},{"cell_type":"markdown","source":"### Encoding the sequence classification targets\n\nTo do so, we build a simple mapping from the auxiliary files.","metadata":{}},{"cell_type":"code","source":"intent_names = Path('/kaggle/input/softcomp/vocab.intent').read_text('utf-8').split()\nintent_map = dict((label, idx) for idx, label in enumerate(intent_names))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T14:10:30.800051Z","iopub.execute_input":"2024-11-17T14:10:30.800324Z","iopub.status.idle":"2024-11-17T14:10:30.813703Z","shell.execute_reply.started":"2024-11-17T14:10:30.800294Z","shell.execute_reply":"2024-11-17T14:10:30.812682Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"intent_map","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T14:10:30.817138Z","iopub.execute_input":"2024-11-17T14:10:30.817549Z","iopub.status.idle":"2024-11-17T14:10:30.823388Z","shell.execute_reply.started":"2024-11-17T14:10:30.817505Z","shell.execute_reply":"2024-11-17T14:10:30.822384Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"{'AddToPlaylist': 0,\n 'BookRestaurant': 1,\n 'GetWeather': 2,\n 'PlayMusic': 3,\n 'RateBook': 4,\n 'SearchCreativeWork': 5,\n 'SearchScreeningEvent': 6}"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"intent_train = df_train['intent_label'].map(intent_map).values\nintent_validation = df_validation['intent_label'].map(intent_map).values\nintent_test = df_test['intent_label'].map(intent_map).values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T14:10:30.824653Z","iopub.execute_input":"2024-11-17T14:10:30.824981Z","iopub.status.idle":"2024-11-17T14:10:30.836511Z","shell.execute_reply.started":"2024-11-17T14:10:30.824940Z","shell.execute_reply":"2024-11-17T14:10:30.835193Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"### Loading and feeding a pretrained BERT model\n\nLet's load a pretrained BERT model using the [huggingface transformers](https://github.com/huggingface/transformers) package.","metadata":{}},{"cell_type":"code","source":"from transformers import TFBertModel\n\n# Replace 'bert-base-cased' with your local model path\nmodel_path = \"/kaggle/input/bert/tensorflow2/default/1/bert\"  # Adjust this path if needed\n\n# Load the model from the local directory\nbase_bert_model = TFBertModel.from_pretrained(model_path)\nbase_bert_model.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T14:10:30.837788Z","iopub.execute_input":"2024-11-17T14:10:30.838121Z","iopub.status.idle":"2024-11-17T14:10:34.310079Z","shell.execute_reply.started":"2024-11-17T14:10:30.838090Z","shell.execute_reply":"2024-11-17T14:10:34.309212Z"}},"outputs":[{"name":"stderr","text":"Some layers from the model checkpoint at /kaggle/input/bert/tensorflow2/default/1/bert were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at /kaggle/input/bert/tensorflow2/default/1/bert.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"},{"name":"stdout","text":"Model: \"tf_bert_model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n bert (TFBertMainLayer)      multiple                  108310272 \n                                                                 \n=================================================================\nTotal params: 108310272 (413.17 MB)\nTrainable params: 108310272 (413.17 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"outputs = base_bert_model(encoded_validation)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T14:10:34.311300Z","iopub.execute_input":"2024-11-17T14:10:34.311681Z","iopub.status.idle":"2024-11-17T14:10:36.386504Z","shell.execute_reply.started":"2024-11-17T14:10:34.311645Z","shell.execute_reply":"2024-11-17T14:10:36.385617Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"The first output of the BERT model is a tensor with shape: `(batch_size, seq_len, output_dim)` which computes features for each token in the input sequence.","metadata":{}},{"cell_type":"markdown","source":"Let's build an train a sequece classification model using to predict the intent class. We will use the `self.bert` pretrained model in the `call` method and only consider the pooled features (ignore the token-wise features for now).","metadata":{}},{"cell_type":"code","source":"# Define IntentClassification model\nclass IntentClassificationModel(tf.keras.Model):\n    def __init__(self, intent_num_labels=None,\n                 model_name='/kaggle/input/bert/tensorflow2/default/1/bert',\n                 dropout_prob=0.1):\n        super().__init__(name='joint_intent_slot')\n        # Let's preload the pretrained model BERT in the constructor\n        # of our classifier model.\n        self.bert = TFBertModel.from_pretrained(model_name)\n        self.dropout = Dropout(dropout_prob)\n        \n        # Define a (Dense) classification layer to compute for each\n        # sequence in a batch of samples. The number of output classes\n        # is given by the intent_num_labels parameter.\n        # Use the default linear activation (no softmax) to compute\n        # logits. The softmax normalization will be computed in the\n        # loss function instead of the model itself.\n        self.intent_classifier = Dense(intent_num_labels)\n        \n    def call(self, inputs, **kwargs):\n        # Extract the features using the pretrained BERT model.\n        bert_output = self.bert(inputs, **kwargs)\n        \n        # The `bert_output` is now an object, so access `pooler_output` directly.\n        pooled_output = bert_output.pooler_output  # (batch_size, hidden_size)\n    \n        # Apply dropout to the pooled output.\n        pooled_output = self.dropout(pooled_output, training=kwargs.get('training', False))\n        \n        # Compute logits using the classifier.\n        intent_logits = self.intent_classifier(pooled_output)\n        \n        return intent_logits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T14:10:36.387621Z","iopub.execute_input":"2024-11-17T14:10:36.387897Z","iopub.status.idle":"2024-11-17T14:10:36.395680Z","shell.execute_reply.started":"2024-11-17T14:10:36.387867Z","shell.execute_reply":"2024-11-17T14:10:36.394832Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"The classification model outputs logits instead of probabilities. The final softmax normalization layer is implicit, that is, included in the loss function instead of the model directly. We need to configure the loss function `SparseCategoricalCrossentropy(from_logits=True)` accordingly.","metadata":{}},{"cell_type":"code","source":"# Build the model\nintent_model = IntentClassificationModel(intent_num_labels=len(intent_map))\n\nintent_model.compile(optimizer=Adam(learning_rate=3e-5, epsilon=1e-08),\n                     loss=SparseCategoricalCrossentropy(from_logits=True),\n                     metrics=[SparseCategoricalAccuracy('accuracy')])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T14:10:36.398652Z","iopub.execute_input":"2024-11-17T14:10:36.399145Z","iopub.status.idle":"2024-11-17T14:10:39.276245Z","shell.execute_reply.started":"2024-11-17T14:10:36.399112Z","shell.execute_reply":"2024-11-17T14:10:39.275315Z"}},"outputs":[{"name":"stderr","text":"Some layers from the model checkpoint at /kaggle/input/bert/tensorflow2/default/1/bert were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at /kaggle/input/bert/tensorflow2/default/1/bert.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# Train the model\nhistory = intent_model.fit(encoded_train, intent_train,\n                           epochs=2, batch_size=32,\n                           validation_data=(encoded_validation, intent_validation))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T14:10:39.277543Z","iopub.execute_input":"2024-11-17T14:10:39.277917Z","iopub.status.idle":"2024-11-17T14:11:40.360932Z","shell.execute_reply.started":"2024-11-17T14:10:39.277872Z","shell.execute_reply":"2024-11-17T14:11:40.359902Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/2\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/layer.py:1295: UserWarning: Layer 'joint_intent_slot' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\nException encountered: ''The following keyword arguments are not supported by this model: ['kwargs'].''\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/keras/src/layers/layer.py:361: UserWarning: `build()` was called on layer 'joint_intent_slot', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n  warnings.warn(\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1731852651.237036      94 service.cc:145] XLA service 0x7d5cd8004bc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1731852651.237089      94 service.cc:153]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nW0000 00:00:1731852651.762363      94 assert_op.cc:38] Ignoring Assert operator joint_intent_slot_1/tf_bert_model_2/bert/embeddings/assert_less/Assert/Assert\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m  3/409\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 43ms/step - accuracy: 0.1719 - loss: 2.5388   ","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1731852654.996674      94 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m407/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.1745 - loss: 2.0616","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1731852672.214821      95 assert_op.cc:38] Ignoring Assert operator joint_intent_slot_1/tf_bert_model_2/bert/embeddings/assert_less/Assert/Assert\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.1747 - loss: 2.0610","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1731852677.899957      95 assert_op.cc:38] Ignoring Assert operator joint_intent_slot_1/tf_bert_model_2/bert/embeddings/assert_less/Assert/Assert\nW0000 00:00:1731852680.773373      94 assert_op.cc:38] Ignoring Assert operator joint_intent_slot_1/tf_bert_model_2/bert/embeddings/assert_less/Assert/Assert\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 67ms/step - accuracy: 0.1748 - loss: 2.0608 - val_accuracy: 0.2957 - val_loss: 1.8767\nEpoch 2/2\n\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 44ms/step - accuracy: 0.3306 - loss: 1.8490 - val_accuracy: 0.3714 - val_loss: 1.8110\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"def classify(text, tokenizerzer, model, intent_names):\n    inputs = tf.constant(tokenizer.encode(text))[None, :] # Batch size = 1\n    class_id = model(inputs).numpy().argmax(axis=1)[0]\n    return intent_names[class_id]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T14:14:12.249778Z","iopub.execute_input":"2024-11-17T14:14:12.250216Z","iopub.status.idle":"2024-11-17T14:14:12.255933Z","shell.execute_reply.started":"2024-11-17T14:14:12.250177Z","shell.execute_reply":"2024-11-17T14:14:12.255009Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# Example of classification\nclassify('Will it snow tomorrow in Paris?',\n         tokenizer, intent_model, intent_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T14:14:12.960672Z","iopub.execute_input":"2024-11-17T14:14:12.961038Z","iopub.status.idle":"2024-11-17T14:14:13.162497Z","shell.execute_reply.started":"2024-11-17T14:14:12.961002Z","shell.execute_reply":"2024-11-17T14:14:13.161636Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"'GetWeather'"},"metadata":{}}],"execution_count":32},{"cell_type":"markdown","source":"## Join intent classification and slot filling\n\nLet's now refine our natural language understanding system by trying to retrieve the important structured elements of each voice command. To do so, we will perform word level (or token level) classification of the BIO labels. Since we have word level tags but BERT uses a wordpiece tokenizer, we need to align the BIO labels with the BERT tokens. Let's load the list of possible word token labels and augment it with an additional padding label to be able to ignore special tokens.","metadata":{}},{"cell_type":"code","source":"slot_names = [\"[PAD]\"]\nslot_names += Path('/kaggle/input/softcomp/vocab.slot').read_text('utf-8').strip().splitlines()\n\nslot_map = {}\nfor label in slot_names:\n    slot_map[label] = len(slot_map)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T14:14:14.237598Z","iopub.execute_input":"2024-11-17T14:14:14.237980Z","iopub.status.idle":"2024-11-17T14:14:14.249041Z","shell.execute_reply.started":"2024-11-17T14:14:14.237945Z","shell.execute_reply":"2024-11-17T14:14:14.248058Z"}},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"The following function generates token-aligned integer labels from the BIO word-level annotations. In particular, if a specific word is too long to be represented as a single token, we expand its label for all the tokens of that word while taking care of using \"B-\" labels only for the first token and then use \"I-\" for the matching slot type for subsequent tokens of the same word.","metadata":{}},{"cell_type":"code","source":"def encode_token_labels(text_sequences, slot_names, tokenizer, slot_map, max_length):\n    encoded = np.zeros(shape=(len(text_sequences), max_length), dtype=np.int32)\n    for i, (text_sequence, word_labels) in enumerate(\n            zip(text_sequences, slot_names)):\n        encoded_labels = []\n        for word, word_label in zip(text_sequence.split(), word_labels.split()):\n            tokens = tokenizer.tokenize(word)\n            encoded_labels.append(slot_map[word_label])\n            expand_label = word_label.replace(\"B-\", \"I-\")\n            if not expand_label in slot_map:\n                expand_label = word_label\n            encoded_labels.extend([slot_map[expand_label]] * (len(tokens) - 1))\n        encoded[i, 1:len(encoded_labels) + 1] = encoded_labels\n    return encoded","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T14:14:15.766993Z","iopub.execute_input":"2024-11-17T14:14:15.767399Z","iopub.status.idle":"2024-11-17T14:14:15.775255Z","shell.execute_reply.started":"2024-11-17T14:14:15.767359Z","shell.execute_reply":"2024-11-17T14:14:15.774151Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"slot_train = encode_token_labels(df_train['words'], df_train['words_label'], tokenizer, slot_map, 45)\nslot_validation = encode_token_labels(df_validation['words'], df_validation['words_label'], tokenizer, slot_map, 45)\nslot_test = encode_token_labels(df_test['words'], df_test['words_label'], tokenizer, slot_map, 45)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T14:14:17.751011Z","iopub.execute_input":"2024-11-17T14:14:17.751644Z","iopub.status.idle":"2024-11-17T14:14:23.923384Z","shell.execute_reply.started":"2024-11-17T14:14:17.751605Z","shell.execute_reply":"2024-11-17T14:14:23.922572Z"}},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":"Note that the special tokens such as \"[PAD]\" and \"[SEP]\" and all padded positions receive a $0$ label.","metadata":{}},{"cell_type":"code","source":"# Define JointIntentAndSlotFilling model\nclass JointIntentAndSlotFillingModel(tf.keras.Model):\n\n    def __init__(self, intent_num_labels=None, slot_num_labels=None,\n                 model_name=\"/kaggle/input/bert/tensorflow2/default/1/bert\", dropout_prob=0.1):\n        super().__init__(name=\"joint_intent_slot\")\n        self.bert = TFBertModel.from_pretrained(model_name)\n        self.dropout = Dropout(dropout_prob)\n        self.intent_classifier = Dense(intent_num_labels,\n                                       name=\"intent_classifier\")\n        self.slot_classifier = Dense(slot_num_labels,\n                                     name=\"slot_classifier\")\n\n    def call(self, inputs, **kwargs):\n        # Get the output from the BERT model, which returns an object.\n        bert_output = self.bert(inputs, **kwargs)\n        \n        # Extract `last_hidden_state` and `pooler_output` from the output object.\n        sequence_output = bert_output.last_hidden_state  # (batch_size, max_length, output_dim)\n        pooled_output = bert_output.pooler_output        # (batch_size, hidden_size)\n    \n        # Apply dropout to the sequence output for slot classification.\n        sequence_output = self.dropout(sequence_output, training=kwargs.get(\"training\", False))\n        slot_logits = self.slot_classifier(sequence_output)\n    \n        # Apply dropout to the pooled output for intent classification.\n        pooled_output = self.dropout(pooled_output, training=kwargs.get(\"training\", False))\n        intent_logits = self.intent_classifier(pooled_output)\n    \n        return slot_logits, intent_logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T14:14:23.925118Z","iopub.execute_input":"2024-11-17T14:14:23.925452Z","iopub.status.idle":"2024-11-17T14:14:23.934407Z","shell.execute_reply.started":"2024-11-17T14:14:23.925419Z","shell.execute_reply":"2024-11-17T14:14:23.933488Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"joint_model = JointIntentAndSlotFillingModel(\n    intent_num_labels=len(intent_map), slot_num_labels=len(slot_map))\n\n# Define one classification loss for each output:\nopt = Adam(learning_rate=3e-5, epsilon=1e-08)\nlosses = [SparseCategoricalCrossentropy(from_logits=True),\n          SparseCategoricalCrossentropy(from_logits=True)]\nmetrics = [SparseCategoricalAccuracy('accuracy'),SparseCategoricalAccuracy('accuracy')]\njoint_model.compile(optimizer=opt, loss=losses, metrics=metrics)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T14:14:23.935621Z","iopub.execute_input":"2024-11-17T14:14:23.936408Z","iopub.status.idle":"2024-11-17T14:14:27.262014Z","shell.execute_reply.started":"2024-11-17T14:14:23.936374Z","shell.execute_reply":"2024-11-17T14:14:27.261304Z"}},"outputs":[{"name":"stderr","text":"Some layers from the model checkpoint at /kaggle/input/bert/tensorflow2/default/1/bert were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nAll the layers of TFBertModel were initialized from the model checkpoint at /kaggle/input/bert/tensorflow2/default/1/bert.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"history = joint_model.fit(\n    encoded_train, (slot_train, intent_train),\n    validation_data=(encoded_validation, (slot_validation, intent_validation)),\n    epochs=20, batch_size=32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T14:16:44.249133Z","iopub.execute_input":"2024-11-17T14:16:44.250217Z","iopub.status.idle":"2024-11-17T14:22:49.150381Z","shell.execute_reply.started":"2024-11-17T14:16:44.250142Z","shell.execute_reply":"2024-11-17T14:22:49.149535Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20\n\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 44ms/step - accuracy: 0.8147 - accuracy_1: 0.3682 - loss: 3.5977 - val_accuracy: 0.8218 - val_accuracy_1: 0.4129 - val_loss: 3.1166\nEpoch 2/20\n\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 44ms/step - accuracy: 0.8281 - accuracy_1: 0.4236 - loss: 2.9908 - val_accuracy: 0.8271 - val_accuracy_1: 0.4157 - val_loss: 2.7652\nEpoch 3/20\n\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 44ms/step - accuracy: 0.8332 - accuracy_1: 0.4490 - loss: 2.6819 - val_accuracy: 0.8342 - val_accuracy_1: 0.4771 - val_loss: 2.5559\nEpoch 4/20\n\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 44ms/step - accuracy: 0.8418 - accuracy_1: 0.4763 - loss: 2.4899 - val_accuracy: 0.8445 - val_accuracy_1: 0.4486 - val_loss: 2.4106\nEpoch 5/20\n\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 44ms/step - accuracy: 0.8521 - accuracy_1: 0.5014 - loss: 2.3425 - val_accuracy: 0.8557 - val_accuracy_1: 0.4971 - val_loss: 2.2901\nEpoch 6/20\n\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 44ms/step - accuracy: 0.8639 - accuracy_1: 0.5235 - loss: 2.2306 - val_accuracy: 0.8662 - val_accuracy_1: 0.5043 - val_loss: 2.1918\nEpoch 7/20\n\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 44ms/step - accuracy: 0.8728 - accuracy_1: 0.5521 - loss: 2.1340 - val_accuracy: 0.8749 - val_accuracy_1: 0.5314 - val_loss: 2.1023\nEpoch 8/20\n\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 44ms/step - accuracy: 0.8814 - accuracy_1: 0.5582 - loss: 2.0513 - val_accuracy: 0.8823 - val_accuracy_1: 0.5400 - val_loss: 2.0265\nEpoch 9/20\n\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 44ms/step - accuracy: 0.8905 - accuracy_1: 0.5810 - loss: 1.9717 - val_accuracy: 0.8898 - val_accuracy_1: 0.5871 - val_loss: 1.9569\nEpoch 10/20\n\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 44ms/step - accuracy: 0.8970 - accuracy_1: 0.5996 - loss: 1.9043 - val_accuracy: 0.8960 - val_accuracy_1: 0.5943 - val_loss: 1.8933\nEpoch 11/20\n\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 44ms/step - accuracy: 0.9044 - accuracy_1: 0.6127 - loss: 1.8411 - val_accuracy: 0.9026 - val_accuracy_1: 0.5800 - val_loss: 1.8351\nEpoch 12/20\n\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 44ms/step - accuracy: 0.9102 - accuracy_1: 0.6216 - loss: 1.7838 - val_accuracy: 0.9079 - val_accuracy_1: 0.6129 - val_loss: 1.7806\nEpoch 13/20\n\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 45ms/step - accuracy: 0.9154 - accuracy_1: 0.6324 - loss: 1.7397 - val_accuracy: 0.9136 - val_accuracy_1: 0.6486 - val_loss: 1.7332\nEpoch 14/20\n\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 46ms/step - accuracy: 0.9194 - accuracy_1: 0.6490 - loss: 1.6968 - val_accuracy: 0.9181 - val_accuracy_1: 0.6214 - val_loss: 1.6885\nEpoch 15/20\n\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 46ms/step - accuracy: 0.9232 - accuracy_1: 0.6530 - loss: 1.6573 - val_accuracy: 0.9214 - val_accuracy_1: 0.6314 - val_loss: 1.6442\nEpoch 16/20\n\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 46ms/step - accuracy: 0.9284 - accuracy_1: 0.6597 - loss: 1.6094 - val_accuracy: 0.9251 - val_accuracy_1: 0.6600 - val_loss: 1.6049\nEpoch 17/20\n\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 46ms/step - accuracy: 0.9309 - accuracy_1: 0.6665 - loss: 1.5737 - val_accuracy: 0.9279 - val_accuracy_1: 0.6700 - val_loss: 1.5693\nEpoch 18/20\n\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 46ms/step - accuracy: 0.9343 - accuracy_1: 0.6838 - loss: 1.5296 - val_accuracy: 0.9312 - val_accuracy_1: 0.6857 - val_loss: 1.5343\nEpoch 19/20\n\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 46ms/step - accuracy: 0.9377 - accuracy_1: 0.6860 - loss: 1.5018 - val_accuracy: 0.9347 - val_accuracy_1: 0.6814 - val_loss: 1.5007\nEpoch 20/20\n\u001b[1m409/409\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 46ms/step - accuracy: 0.9390 - accuracy_1: 0.6871 - loss: 1.4773 - val_accuracy: 0.9372 - val_accuracy_1: 0.6786 - val_loss: 1.4726\n","output_type":"stream"}],"execution_count":44},{"cell_type":"markdown","source":"The following function uses our trained model to make prediction on a single text sequence and display both the sequence-wise and the token-wise class labels.","metadata":{}},{"cell_type":"code","source":"def show_predictions(text, tokenizer, model, intent_names, slot_names):\n    inputs = tf.constant(tokenizer.encode(text))[None, :]  # batch_size = 1\n    outputs = model(inputs)\n    slot_logits, intent_logits = outputs\n    slot_ids = slot_logits.numpy().argmax(axis=-1)[0, 1:-1]\n    intent_id = intent_logits.numpy().argmax(axis=-1)[0]\n    print(\"## Intent:\", intent_names[intent_id])\n    print(\"## Slots:\")\n    for token, slot_id in zip(tokenizer.tokenize(text), slot_ids):\n        print(f\"{token:>10} : {slot_names[slot_id]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T14:22:53.707186Z","iopub.execute_input":"2024-11-17T14:22:53.708102Z","iopub.status.idle":"2024-11-17T14:22:53.715122Z","shell.execute_reply.started":"2024-11-17T14:22:53.708059Z","shell.execute_reply":"2024-11-17T14:22:53.714133Z"}},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":"## Decoding predictions into structured knowledge\n\nFor completeness, here a minimal functional to naively decode the predicted BIO slot ids and convert it into a structured representation for the detected slots as a Python dictionaries.","metadata":{}},{"cell_type":"code","source":"def decode_predictions(text, tokenizer, intent_names, slot_names,\n                       intent_id, slot_ids):\n    info = {\"intent\": intent_names[intent_id]}\n    collected_slots = {}\n    active_slot_words = []\n    active_slot_name = None\n    for word in text.split():\n        tokens = tokenizer.tokenize(word)\n        current_word_slot_ids = slot_ids[:len(tokens)]\n        slot_ids = slot_ids[len(tokens):]\n        current_word_slot_name = slot_names[current_word_slot_ids[0]]\n        if current_word_slot_name == \"O\":\n            if active_slot_name:\n                collected_slots[active_slot_name] = \" \".join(active_slot_words)\n                active_slot_words = []\n                active_slot_name = None\n        else:\n            # Naive BIO: handling: treat B- and I- the same...\n            new_slot_name = current_word_slot_name[2:]\n            if active_slot_name is None:\n                active_slot_words.append(word)\n                active_slot_name = new_slot_name\n            elif new_slot_name == active_slot_name:\n                active_slot_words.append(word)\n            else:\n                collected_slots[active_slot_name] = \" \".join(active_slot_words)\n                active_slot_words = [word]\n                active_slot_name = new_slot_name\n    if active_slot_name:\n        collected_slots[active_slot_name] = \" \".join(active_slot_words)\n    info[\"slots\"] = collected_slots\n    return info","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T14:23:03.573035Z","iopub.execute_input":"2024-11-17T14:23:03.573466Z","iopub.status.idle":"2024-11-17T14:23:03.582903Z","shell.execute_reply.started":"2024-11-17T14:23:03.573425Z","shell.execute_reply":"2024-11-17T14:23:03.581898Z"}},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":"## Machine Translation Hindi to English\n","metadata":{}},{"cell_type":"code","source":"\n# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\ntokenizer_trans = AutoTokenizer.from_pretrained(\"TestZee/FineTuned-hindi-to-english-V8\")\nmodel_trans = AutoModelForSeq2SeqLM.from_pretrained(\"TestZee/FineTuned-hindi-to-english-V8\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T14:23:10.840671Z","iopub.execute_input":"2024-11-17T14:23:10.841557Z","iopub.status.idle":"2024-11-17T14:23:21.099573Z","shell.execute_reply.started":"2024-11-17T14:23:10.841513Z","shell.execute_reply":"2024-11-17T14:23:21.098597Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/323 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42bfea9159994a6cade31aa3b381b608"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"source.spm:   0%|          | 0.00/1.06M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"729292f9281e44f2b26c3646890158d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"target.spm:   0%|          | 0.00/813k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e74de17e879c48ae96db990ba4ea2b78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.18M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"096454434ce0460ca45a417bb6c1c236"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/74.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"242c9aacf3394f35ae0750756e361ae2"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n  warnings.warn(\"Recommended: pip install sacremoses.\")\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"190b8ec57e704990bd4a4e4f53310cec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/302M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23e926c3e03b4b8e8d46f858d17f3c94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/288 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"551ee14d96f24529a05b1c4c380d796b"}},"metadata":{}}],"execution_count":48},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"def nlu(text, tokenizer, model, intent_names, slot_names):\n    tokenized_text = tokenizer_trans(text,return_tensors=\"pt\")\n    translated = model_trans.generate(**tokenized_text)\n    text = tokenizer_trans.batch_decode(translated, skip_special_tokens=True)[0]\n    inputs = tf.constant(tokenizer.encode(text))[None, :]  # batch_size = 1\n    outputs = model(inputs)\n    slot_logits, intent_logits = outputs\n    slot_ids = slot_logits.numpy().argmax(axis=-1)[0, 1:-1]\n    intent_id = intent_logits.numpy().argmax(axis=-1)[0]\n\n    return decode_predictions(text, tokenizer, intent_names, slot_names,\n                              intent_id, slot_ids)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T14:23:21.243738Z","iopub.execute_input":"2024-11-17T14:23:21.243995Z","iopub.status.idle":"2024-11-17T14:23:21.251131Z","shell.execute_reply.started":"2024-11-17T14:23:21.243963Z","shell.execute_reply":"2024-11-17T14:23:21.250229Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"nlu('में इतनी महंगी किताब नहीं खरीद सकता ।',\n                 tokenizer, joint_model, intent_names, slot_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T14:24:25.235387Z","iopub.execute_input":"2024-11-17T14:24:25.236012Z","iopub.status.idle":"2024-11-17T14:24:26.053943Z","shell.execute_reply.started":"2024-11-17T14:24:25.235964Z","shell.execute_reply":"2024-11-17T14:24:26.052911Z"}},"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"{'intent': 'RateBook', 'slots': {'sort': 'expensive', 'object_type': 'books.'}}"},"metadata":{}}],"execution_count":52},{"cell_type":"code","source":"nlu('क्या आज मनाली में बर्फबारी होगी?',\n    tokenizer, joint_model, intent_names, slot_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T14:33:00.873674Z","iopub.execute_input":"2024-11-17T14:33:00.874630Z","iopub.status.idle":"2024-11-17T14:33:01.588322Z","shell.execute_reply.started":"2024-11-17T14:33:00.874588Z","shell.execute_reply":"2024-11-17T14:33:01.587435Z"}},"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"{'intent': 'GetWeather',\n 'slots': {'condition_description': 'snowfalls',\n  'city': 'Nitali',\n  'timeRange': 'today?'}}"},"metadata":{}}],"execution_count":64},{"cell_type":"markdown","source":"## Limitations\n\n### Language\n\nBERT is pretrained primarily on English content. It can therefore only extract meaningful features on text written in English.\n\nNote that there exists alternative pretrained model that use a mix of different languages (e.g. [XLM](https://github.com/facebookresearch/XLM/) and others that have been trained on other languages. For instance [CamemBERT](https://camembert-model.fr/) is pretrained on French text. Both kinds of models are available in the transformers package:\n\nhttps://github.com/huggingface/transformers#model-architectures\n\nThe public snips.ai dataset used for fine-tuning is English only. To build a model for another language we would need to collect and annotate a similar corpus with tens of thousands of diverse, representative samples.\n\n### Biases embedded in the pre-trained model\n\nThe original data used to pre-trained BERT was collected from the Internet and contains all kinds of data, including offensive and hateful speech.\n\nWhile using BERT for or voice command understanding system is quite unlikely to be significantly impacted by those biases, it could be a serious problem for other kinds of applications such as Machine Translation for instance.\n\nIt is therefore strongly recommended to spend time auditing the biases that are embedded in such pre-trained models before deciding to deploy system that derive from them.\n\n### Computational ressources\n\nThe original BERT model has many parameters which uses a lot of memory and can be prohibitive to deploy on small devices such as mobile phones. It is also very computationally intensive and typically requires powerful GPUs or TPUs to process text data at a reasonable speed (both for training and at inference time).\n\nDesigning alternative architectures with fewer parameters or more efficient training and inference procedures is still a very active area of research.\n\nDepending of on the problems, it might be the case that simpler architectures based on convolutional neural networks and LSTMs might offer a better speed / accuracy trade-off.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}